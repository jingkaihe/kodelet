# Kodelet Sample Configuration File
# This file shows all available configuration options with example values

# Logging Configuration
# Log level for the application (panic, fatal, error, warn, info, debug, trace)
log_level: "info"

# Log format for the application (json, text, fmt)
log_format: "json"

# LLM Configuration
# Provider to use (anthropic, openai, or google)
provider: "anthropic"

# Model to use for LLM interactions
model: "claude-sonnet-4-5-20250929"

# Maximum tokens for responses
max_tokens: 8192

# Maximum tokens budget for thinking capability
thinking_budget_tokens: 4048

# Weak model to use for less complex tasks
weak_model: "claude-haiku-4-5-20251001"

# Maximum tokens for weak model responses
weak_model_max_tokens: 8192

# Model aliases for easier reference
# Allows using short names instead of full model identifiers
aliases:
  sonnet-45: "claude-sonnet-4-5-20250929"
  haiku-45: "claude-haiku-4-5-20251001"
  opus-4: "claude-opus-4-20250514"
  gemini-pro: "gemini-2.5-pro"
  gemini-flash: "gemini-2.5-flash"

# Active profile selection (uncomment to activate a specific profile)
# profile: "premium"

# Profile definitions for different use cases
# Each profile can override any configuration setting
profiles:
  premium:
    model: "claude-opus-4-20250514"
    weak_model: "claude-sonnet-4-5-20250929"
    max_tokens: 16000
    weak_model_max_tokens: 8192
    thinking_budget_tokens: 8000

  fast:
    model: "haiku-45"
    weak_model: "haiku-45"
    max_tokens: 4096
    weak_model_max_tokens: 4096
    thinking_budget_tokens: 2000

  openai:
    provider: "openai"
    use_copilot: true
    model: "gpt-4.1"
    weak_model: "gpt-4.1-mini"
    max_tokens: 16000
    reasoning_effort: "medium"

  xai:
    provider: "openai"
    model: "grok-3"
    weak_model: "grok-3-mini"
    max_tokens: 16000
    reasoning_effort: "none"
    openai:
      preset: "xai"

  google-pro:
    provider: "google"
    model: "gemini-pro"
    weak_model: "gemini-flash"
    max_tokens: 8192
    thinking_budget_tokens: 8000

  mix-n-match:
    provider: "anthropic"
    model: "sonnet-45"
    weak_model: "haiku-45"
    max_tokens: 16000

    subagent:
      provider: "openai"
      model: "o3"
      reasoning_effort: "high"
      allowed_tools: ["file_read", "glob_tool", "grep_tool", "thinking"]

# OpenAI specific settings
# Reasoning effort for OpenAI models (low, medium, high)
reasoning_effort: "medium"

# API Retry Configuration
# Controls retry behavior for API calls
# - Anthropic: Only 'attempts' is used (relies on SDK's built-in retry)
# - OpenAI: All fields are used (custom retry implementation)
retry:
  # Maximum number of retry attempts (default: 3)
  # Used by both Anthropic and OpenAI
  attempts: 3

  # Initial delay in milliseconds before first retry (default: 1000)
  # Used by OpenAI only
  initial_delay: 1000

  # Maximum delay in milliseconds between retries (default: 10000)
  # Used by OpenAI only
  max_delay: 10000

  # Backoff strategy: "fixed" or "exponential" (default: "exponential")
  # Used by OpenAI only
  backoff_type: "exponential"

# Security Configuration
# Command allow list for bash tool (empty means use default banned commands)
# When specified, only commands matching these patterns are allowed
# Supports wildcards: * matches any string
# Example patterns:
#   - "ls": exact match for ls command
#   - "ls *": ls with any arguments
#   - "npm *": any npm command
#   - "echo hello": exact echo hello command
allowed_commands: []
# allowed_commands:
#   - "ls *"
#   - "pwd"
#   - "echo *"
#   - "cat *"
#   - "grep *"
#   - "find *"
#   - "npm *"
#   - "yarn *"
#   - "git status"
#   - "git log *"

# Domain Filtering Configuration
# Path to file containing allowed domains for web_fetch tool (one domain per line)
# Supports exact matches (github.com) and glob patterns (*.github.com)
# If the file doesn't exist or is empty, all domains are allowed
# Localhost addresses (127.0.0.1, localhost, etc.) are always allowed
allowed_domains_file: "~/.kodelet/allowed_domains.txt"

# Tool Configuration
# Tools allowed for main agent (empty means use defaults)
# Meta tools (file_read, grep_tool, glob_tool, thinking) are always enabled for basic functionality
# Available tools: bash, file_read, file_write, file_edit, thinking, subagent, grep_tool,
#                  glob_tool, todo_read, todo_write, web_fetch, image_recognition,
#                  view_background_processes
allowed_tools: []
# allowed_tools:
#   - "bash"
#   - "file_read"      # Always enabled (meta tool)
#   - "file_write"
#   - "file_edit"
#   - "thinking"       # Always enabled (meta tool)
#   - "subagent"
#   - "grep_tool"      # Always enabled (meta tool)
#   - "glob_tool"      # Always enabled (meta tool)
#   - "todo_read"
#   - "todo_write"
#   - "web_fetch"
#   - "image_recognition"

# Commit Configuration
# Configure coauthor attribution for commit messages
commit:
  coauthor:
    # Enable or disable coauthor attribution (default: true)
    enabled: true
    # Coauthor name (default: "Kodelet")
    name: "Kodelet"
    # Coauthor email (default: "noreply@kodelet.com")
    email: "noreply@kodelet.com"


# Example OpenAI configuration (uncomment to use)
# provider: "openai"
# model: "gpt-4.1"
# max_tokens: 8192
# weak_model: "gpt-4.1-mini"
# weak_model_max_tokens: 4096
# reasoning_effort: "medium"

# Example Google GenAI configuration (uncomment to use)
# provider: "google"
# model: "gemini-2.5-pro"
# max_tokens: 8192
# weak_model: "gemini-2.5-flash"
# weak_model_max_tokens: 4096
# thinking_budget_tokens: 8000

# OpenAI-compatible API configuration
# Supports any OpenAI-compatible provider (xAI, Groq, Together AI, etc.)
openai:
  # Option 1: Use a built-in preset (recommended for popular providers)
  # preset: "xai"  # Built-in preset for xAI's Grok models

  # Option 2: Custom configuration (overrides preset if both are specified)
  # base_url: "https://api.x.ai/v1"  # Custom API endpoint
  # api_key_env_var: "XAI_API_KEY"   # Environment variable name for API key (defaults to OPENAI_API_KEY)
  # models:
  #   # Models that support reasoning capabilities (o1, o3, grok reasoning models, etc.)
  #   reasoning:
  #     - "grok-4-0709"
  #     - "grok-3-mini"
  #     - "grok-3-mini-fast"
  #   # Non-reasoning models (auto-populated if not specified)
  #   non_reasoning:
  #     - "grok-3"
  #     - "grok-3-fast"
  #     - "grok-2-vision-1212"
  # pricing:
  #   grok-4-0709:
  #     input: 0.000003         # $3 per million tokens
  #     output: 0.000015        # $15 per million tokens
  #     context_window: 256000  # 256k tokens
  #   grok-3:
  #     input: 0.000003         # $3 per million tokens
  #     output: 0.000015        # $15 per million tokens
  #     context_window: 131072  # 131k tokens
  #   # ... additional models

# Google GenAI Configuration
# Supports both Gemini API (developer-focused) and Vertex AI (enterprise-grade)
google:
  # Backend selection: "gemini" (Gemini API) or "vertexai" (Vertex AI)
  # Auto-detected based on available authentication if not specified
  # backend: "gemini"

  # Authentication for Gemini API
  # api_key: "your-google-api-key"  # Can also use GOOGLE_API_KEY environment variable

  # Authentication for Vertex AI (choose one method)
  # project: "your-gcp-project-id"      # GCP project ID
  # location: "us-central1"             # GCP region (default: us-central1)
  # Using GOOGLE_APPLICATION_CREDENTIALS environment variable is recommended

  # Token budget for thinking capability (Google models support thinking)
  # thinking_budget: 8000  # Default: 8000 tokens



  # Model-specific pricing and configuration (advanced users)
  # pricing:
  #   gemini-2.5-pro:
  #     input: 0.00125            # $1.25 per million tokens (≤200K input)
  #     input_high: 0.0025        # $2.50 per million tokens (>200K input)
  #     output: 0.01              # $10 per million tokens (≤200K input)
  #     output_high: 0.015        # $15 per million tokens (>200K input)
  #     context_window: 2097152   # 2M tokens
  #   gemini-2.5-flash:
  #     input: 0.0003             # $0.30 per million tokens
  #     audio_input: 0.001        # $1.00 per million tokens (audio)
  #     output: 0.0025            # $2.50 per million tokens
  #     context_window: 1048576   # 1M tokens

# Subagent Configuration
# Configure how subagents behave, including provider mix-and-match capabilities
# This allows main agent and subagents to use different providers/models
subagent:
  # Provider for subagents (can be different from main agent)
  # Example: Use GPT for subagents while main agent uses Claude
  # provider: "openai"

  # Model for subagents (typically a lighter/faster model)
  # model: "gpt-4o-mini"

  # Maximum tokens for subagent responses
  # max_tokens: 2048

  # OpenAI-specific: Reasoning effort for subagent (low, medium, high)
  # reasoning_effort: "low"

  # Anthropic-specific: Thinking budget for subagent
  # thinking_budget: 1024

  # Google-specific: Thinking budget for subagent
  # thinking_budget: 2048

  # Tools allowed for subagent (empty means use defaults)
  # Note: subagent tool is automatically excluded to prevent infinite recursion
  # Meta tools (file_read, grep_tool, glob_tool, thinking) are always enabled
  # allowed_tools:
  #   - "bash"
  #   - "file_read"      # Always enabled (meta tool)
  #   - "file_write"
  #   - "file_edit"
  #   - "grep_tool"      # Always enabled (meta tool)
  #   - "glob_tool"      # Always enabled (meta tool)
  #   - "thinking"       # Always enabled (meta tool)
  #   - "todo_read"
  #   - "todo_write"
  #   - "web_fetch"

  # OpenAI configuration for subagent (when using different provider)
  # openai:
  #   preset: "openai"
  #   # Or custom configuration:
  #   # base_url: "https://api.openai.com/v1"
  #   # pricing:
  #   #   gpt-4o-mini:
  #   #     input: 0.00015    # $0.15 per million tokens
  #   #     output: 0.0006    # $0.60 per million tokens
  #   #     context_window: 128000

  # Google configuration for subagent (when using different provider)
  # google:
  #   backend: "gemini"
  #   thinking_budget: 2048
  #   # Or custom configuration:
  #   # api_key: "your-google-api-key"
  #   # pricing:
  #   #   gemini-2.5-flash:
  #   #     input: 0.0003     # $0.30 per million tokens
  #   #     output: 0.0025    # $2.50 per million tokens
  #   #     context_window: 1048576

# Example configurations for common scenarios:

# Scenario 1: Claude main agent with GPT subagents for cost optimization
# provider: "anthropic"
# model: "claude-sonnet-4-5-20250929"
# subagent:
#   provider: "openai"
#   model: "gpt-4o-mini"
#   max_tokens: 2048

# Scenario 2: GPT main agent with Claude subagents for specific tasks
# provider: "openai"
# model: "gpt-4.1"
# reasoning_effort: "high"
# subagent:
#   provider: "anthropic"
#   model: "claude-haiku-4-5-20251001"
#   max_tokens: 4096

# Scenario 3: Same provider, different models for performance
# provider: "anthropic"
# model: "claude-opus-4-20250514"
# subagent:
#   model: "claude-haiku-4-5-20251001"  # Faster model for subagent tasks
#   max_tokens: 2048

# Scenario 4: Google main agent with Claude subagents for specific tasks
# provider: "google"
# model: "gemini-2.5-pro"
# thinking_budget_tokens: 8000
# subagent:
#   provider: "anthropic"
#   model: "claude-haiku-4-5-20251001"
#   max_tokens: 4096

# Scenario 5: Claude main agent with Google subagents for cost optimization
# provider: "anthropic"
# model: "claude-sonnet-4-5-20250929"
# subagent:
#   provider: "google"
#   model: "gemini-2.5-flash"
#   max_tokens: 2048

# Scenario 6: Google Vertex AI for enterprise with thinking capability
# provider: "google"
# model: "gemini-2.5-pro"
# thinking_budget_tokens: 8000
# google:
#   backend: "vertexai"
#   project: "my-company-project"
#   location: "us-central1"


# Custom Tools Configuration
# Kodelet can discover and execute custom executable tools from specified directories
custom_tools:
  # Enable/disable custom tools discovery (default: true)
  enabled: true

  # Global custom tools directory (default: ~/.kodelet/tools)
  global_dir: "~/.kodelet/tools"

  # Local custom tools directory (default: ./.kodelet/tools)
  local_dir: "./.kodelet/tools"

  # Execution timeout for custom tools (default: 30s)
  timeout: 30s

  # Maximum output size for custom tools (default: 100KB)
  max_output_size: 102400

  # Tool whitelist - only tools in this list will be loaded (empty means load all tools)
  # When specified, only custom tools with these names will be available
  # tool_white_list:
  #   - "my-custom-tool"
  #   - "another-tool"

# Agentic Skills Configuration
# Skills are model-invoked capabilities that package domain expertise
# Unlike fragments/recipes (user-invoked), skills are automatically invoked by Kodelet
# when relevant to the task. See docs/SKILLS.md for creating custom skills.
skills:
  # Enable/disable skills globally (default: true when not specified)
  # Set to false to completely disable the skills system
  enabled: true

  # Allowlist of skill names (empty = all discovered skills are available)
  # When specified, only these skills will be enabled
  # Skills are discovered from:
  #   - ./.kodelet/skills/<name>/SKILL.md (repository-local, higher precedence)
  #   - ~/.kodelet/skills/<name>/SKILL.md (user-global)
  # allowed:
  #   - pdf
  #   - xlsx
  #   - kubernetes

# Tracing Configuration
tracing:
  # Enable OpenTelemetry tracing (default: false)
  enabled: true

  # Sampling strategy (options: always, never, ratio)
  sampler: always

  # Sampling ratio when using ratio sampler (0.0-1.0)
  ratio: 1

# Environment variables can also be used to configure Kodelet:
# - KODELET_LOG_LEVEL: Overrides the log_level setting
# - KODELET_LOG_FORMAT: Overrides the log_format setting
# - KODELET_PROVIDER: Overrides the provider setting (anthropic, openai, google)
# - KODELET_MODEL: Overrides the model setting
# - KODELET_MAX_TOKENS: Overrides the max_tokens setting
# - KODELET_THINKING_BUDGET_TOKENS: Overrides the thinking_budget_tokens setting
# - KODELET_WEAK_MODEL: Overrides the weak_model setting
# - KODELET_WEAK_MODEL_MAX_TOKENS: Overrides the weak_model_max_tokens setting
# - KODELET_REASONING_EFFORT: Overrides the reasoning_effort setting (OpenAI)
# - KODELET_WEAK_REASONING_EFFORT: Overrides the weak_reasoning_effort setting (OpenAI)
# - KODELET_RETRY_ATTEMPTS: Overrides the retry.attempts setting
# - KODELET_RETRY_INITIAL_DELAY: Overrides the retry.initial_delay setting (milliseconds)
# - KODELET_RETRY_MAX_DELAY: Overrides the retry.max_delay setting (milliseconds)
# - KODELET_RETRY_BACKOFF_TYPE: Overrides the retry.backoff_type setting (fixed, exponential)
# - KODELET_ALLOWED_COMMANDS: Comma-separated list of allowed command patterns
# - KODELET_ALLOWED_TOOLS: Comma-separated list of allowed tools for main agent
# - KODELET_TRACING_ENABLED: Enables/disables tracing
# - KODELET_TRACING_SAMPLER: Sets the sampling strategy
# - KODELET_TRACING_RATIO: Sets the sampling ratio
# - KODELET_ALIASES_*: Define individual aliases (e.g., KODELET_ALIASES_SONNET4=claude-sonnet-4-5-20250929)
# - KODELET_COMMIT_COAUTHOR_ENABLED: Enable/disable coauthor attribution in commits
# - KODELET_COMMIT_COAUTHOR_NAME: Name for coauthor attribution
# - KODELET_COMMIT_COAUTHOR_EMAIL: Email for coauthor attribution
#
# API Keys:
# - ANTHROPIC_API_KEY: Required when using the Anthropic provider
# - OPENAI_API_KEY: Required when using the OpenAI provider (default)
# - XAI_API_KEY: Required when using the xAI preset
# - OPENAI_API_BASE: Custom OpenAI API endpoint (overrides config base_url)
# - GOOGLE_API_KEY: Required when using Google Gemini API
# - GEMINI_API_KEY: Alternative to GOOGLE_API_KEY for Gemini API
# - GOOGLE_CLOUD_PROJECT: GCP project ID for Vertex AI
# - GOOGLE_CLOUD_LOCATION: GCP region for Vertex AI (default: us-central1)
# - GOOGLE_APPLICATION_CREDENTIALS: Path to GCP service account JSON for Vertex AI
# - GOOGLE_GENAI_USE_VERTEXAI: Force Vertex AI backend (true/false)
# Note: The API key environment variable can be customized using the api_key_env_var setting
#
# Standard OpenTelemetry environment variables are also supported:
# - OTEL_EXPORTER_OTLP_ENDPOINT: The endpoint to send telemetry data to
# - OTEL_EXPORTER_OTLP_HEADERS: Headers to use when sending telemetry data (for auth)
# - OTEL_RESOURCE_ATTRIBUTES: Additional resource attributes to include

# MCP (Model Context Protocol) Configuration
mcp:
  # Execution mode for MCP tools
  # - "direct": Traditional direct tool calling (default)
  # - "code": Code execution with filesystem API (more efficient for multi-tool workflows)
  execution_mode: "direct"

  # Code execution settings (only used when execution_mode = "code")
  code_execution:
    # Directory where MCP tool TypeScript files are generated
    workspace_dir: ".kodelet/mcp"

    # Whether to regenerate tool files on startup
    regenerate_on_startup: false

    # Unix socket path for MCP RPC server
    socket_path: ".kodelet/mcp.sock"

  # MCP servers configuration
  # Each server can be either stdio or SSE based
  servers:
    # Example stdio server (uncomment to enable)
    # filesystem:
    #   command: "npx"
    #   args: ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/allowed/directory"]

    # Example SSE server (uncomment to enable)
    # example-sse:
    #   server_type: "sse"
    #   base_url: "https://example.com/mcp"
    #   headers:
    #     Authorization: "Bearer ${MCP_TOKEN}"
    #   tool_white_list: ["tool1", "tool2"]
