# Kodelet Sample Configuration File
# This file shows all available configuration options with example values

# Logging Configuration
# Log level for the application (panic, fatal, error, warn, info, debug, trace)
log_level: "info"

# Log format for the application (json, text, fmt)
log_format: "json"

# LLM Configuration
# Provider to use (anthropic, openai, or google)
provider: "anthropic"

# Model to use for LLM interactions
model: "claude-sonnet-4-6"

# Maximum tokens for responses
max_tokens: 8192

# Maximum tokens budget for thinking capability
thinking_budget_tokens: 4048

# Weak model to use for less complex tasks
weak_model: "claude-haiku-4-5-20251001"

# Maximum tokens for weak model responses
weak_model_max_tokens: 8192

# Custom system prompt template file
# Supports Go templates and can include built-in sections, e.g.:
# {{include "templates/sections/behavior.tmpl" .}}
# {{include "templates/sections/tooling.tmpl" .}}
# {{include "templates/sections/context_runtime.tmpl" .}}
sysprompt: ""

# Arguments available to custom system prompt templates as .Args.<key>
# sysprompt_args:
#   project: "kodelet"
#   env: "dev"
sysprompt_args: {}

# Model aliases for easier reference
# Allows using short names instead of full model identifiers
aliases:
  sonnet-46: "claude-sonnet-4-6"
  haiku-45: "claude-haiku-4-5-20251001"
  opus-46: "claude-opus-4-6"
  gemini-pro: "gemini-2.5-pro"
  gemini-flash: "gemini-2.5-flash"

# Active profile selection (uncomment to activate a specific profile)
# profile: "premium"

# Profile definitions for different use cases
# Each profile can override any configuration setting
profiles:
  premium:
    model: "claude-opus-4-6"
    weak_model: "claude-sonnet-4-6"
    max_tokens: 16000
    weak_model_max_tokens: 8192
    thinking_budget_tokens: 8000

  fast:
    model: "haiku-45"
    weak_model: "haiku-45"
    max_tokens: 4096
    weak_model_max_tokens: 4096
    thinking_budget_tokens: 2000

  openai:
    provider: "openai"
    use_copilot: true
    model: "gpt-5.2-codex"
    weak_model: "gpt-5.2-codex-mini"
    max_tokens: 16000
    reasoning_effort: "medium"
    openai:
      use_responses_api: true

  xai:
    provider: "openai"
    model: "grok-3"
    weak_model: "grok-3-mini"
    max_tokens: 16000
    reasoning_effort: "none"
    openai:
      platform: "xai"

  fireworks:
    provider: "openai"
    model: "accounts/fireworks/models/kimi-k2p5"
    weak_model: "accounts/fireworks/models/kimi-k2p5"
    max_tokens: 32768
    weak_model_max_tokens: 8192
    openai:
      manual_cache: true
      base_url: "https://api.fireworks.ai/inference/v1"
      api_key_env_var: "FIREWORK_API_KEY"
      pricing:
        accounts/fireworks/models/kimi-k2p5:
          input: 0.0000006         # $0.6 per million tokens
          cached_input: 0.0000001  # $0.1 per million tokens
          output: 0.000003         # $3 per million tokens
          context_window: 262000   # 262k tokens

  openai-responses:
    provider: "openai"
    model: "gpt-5.2-codex"
    weak_model: "gpt-5.2-codex-mini"
    max_tokens: 16000
    reasoning_effort: "medium"
    openai:
      use_responses_api: true

  google-pro:
    provider: "google"
    model: "gemini-pro"
    weak_model: "gemini-flash"
    max_tokens: 8192
    thinking_budget_tokens: 8000

  mix-n-match:
    provider: "anthropic"
    model: "sonnet-46"
    weak_model: "haiku-45"
    max_tokens: 16000
    # Use OpenAI subagent profile for cross-provider support
    subagent_args: "--profile openai-subagent"

  # Subagent profile for mix-n-match (cross-provider example)
  openai-subagent:
    provider: "openai"
    model: "o3"
    reasoning_effort: "high"
    allowed_tools: ["file_read", "glob_tool", "grep_tool", "thinking"]

# OpenAI specific settings
# Reasoning effort for OpenAI models (low, medium, high)
reasoning_effort: "medium"

# API Retry Configuration
# Controls retry behavior for API calls
# - Anthropic: Only 'attempts' is used (relies on SDK's built-in retry)
# - OpenAI: All fields are used (custom retry implementation)
retry:
  # Maximum number of retry attempts (default: 3)
  # Used by both Anthropic and OpenAI
  attempts: 3

  # Initial delay in milliseconds before first retry (default: 1000)
  # Used by OpenAI only
  initial_delay: 1000

  # Maximum delay in milliseconds between retries (default: 10000)
  # Used by OpenAI only
  max_delay: 10000

  # Backoff strategy: "fixed" or "exponential" (default: "exponential")
  # Used by OpenAI only
  backoff_type: "exponential"

# Security Configuration
# Command allow list for bash tool (empty means use default banned commands)
# When specified, only commands matching these patterns are allowed
# Supports wildcards: * matches any string
# Example patterns:
#   - "ls": exact match for ls command
#   - "ls *": ls with any arguments
#   - "npm *": any npm command
#   - "echo hello": exact echo hello command
allowed_commands: []
# allowed_commands:
#   - "ls *"
#   - "pwd"
#   - "echo *"
#   - "cat *"
#   - "grep *"
#   - "find *"
#   - "npm *"
#   - "yarn *"
#   - "git status"
#   - "git log *"

# Domain Filtering Configuration
# Path to file containing allowed domains for web_fetch tool (one domain per line)
# Supports exact matches (github.com) and glob patterns (*.github.com)
# If the file doesn't exist or is empty, all domains are allowed
# Localhost addresses (127.0.0.1, localhost, etc.) are always allowed
allowed_domains_file: "~/.kodelet/allowed_domains.txt"

# Tool Configuration
# Tools allowed for main agent (empty means use defaults)
# Meta tools (file_read, grep_tool, glob_tool, thinking) are always enabled for basic functionality
# Available tools: bash, apply_patch, file_read, file_write, file_edit, thinking, subagent, grep_tool,
#                  glob_tool, todo_read, todo_write, web_fetch, image_recognition,
#                  view_background_processes
# Enable apply_patch-first mode. When true, file_write and file_edit are removed from
# both main-agent and subagent toolsets, and apply_patch is ensured.
apply_patch_enabled: false

allowed_tools: []
# allowed_tools:
#   - "bash"
#   - "file_read"      # Always enabled (meta tool)
#   - "file_write"
#   - "file_edit"
#   - "thinking"       # Always enabled (meta tool)
#   - "subagent"
#   - "grep_tool"      # Always enabled (meta tool)
#   - "glob_tool"      # Always enabled (meta tool)
#   - "todo_read"
#   - "todo_write"
#   - "web_fetch"
#   - "image_recognition"

# Commit Configuration
# Configure coauthor attribution for commit messages
commit:
  coauthor:
    # Enable or disable coauthor attribution (default: true)
    enabled: true
    # Coauthor name (default: "Kodelet")
    name: "Kodelet"
    # Coauthor email (default: "noreply@kodelet.com")
    email: "noreply@kodelet.com"


# Example OpenAI configuration (uncomment to use)
# provider: "openai"
# model: "gpt-4.1"
# max_tokens: 8192
# weak_model: "gpt-4.1-mini"
# weak_model_max_tokens: 4096
# reasoning_effort: "medium"

# Example Google GenAI configuration (uncomment to use)
# provider: "google"
# model: "gemini-2.5-pro"
# max_tokens: 8192
# weak_model: "gemini-2.5-flash"
# weak_model_max_tokens: 4096
# thinking_budget_tokens: 8000

# OpenAI-compatible API configuration
# Supports any OpenAI-compatible provider (xAI, Groq, Together AI, etc.)
openai:
  # Optional platform label used for built-in defaults/metadata (e.g., openai, xai, codex, fireworks)
  # platform: "xai"

  # Custom endpoint/auth configuration
  # base_url: "https://api.x.ai/v1"  # Custom API endpoint
  # api_key_env_var: "XAI_API_KEY"   # Environment variable name for API key (defaults to OPENAI_API_KEY)

  # Use the Responses API instead of Chat Completions API (default: false)
  # The Responses API offers multi-turn conversations via previous_response_id,
  # native compaction support, and richer streaming events.
  # use_responses_api: true

  # Enable manual prompt-cache affinity for Chat Completions.
  # When true, kodelet adds x-session-affinity header per conversation,
  # but only on calls where prompt caching is requested.
  # manual_cache: true
  # models:
  #   # Models that support reasoning capabilities (o1, o3, grok reasoning models, etc.)
  #   reasoning:
  #     - "grok-4-0709"
  #     - "grok-3-mini"
  #     - "grok-3-mini-fast"
  #   # Non-reasoning models (auto-populated if not specified)
  #   non_reasoning:
  #     - "grok-3"
  #     - "grok-3-fast"
  #     - "grok-2-vision-1212"
  # pricing:
  #   grok-4-0709:
  #     input: 0.000003         # $3 per million tokens
  #     output: 0.000015        # $15 per million tokens
  #     context_window: 256000  # 256k tokens
  #   grok-3:
  #     input: 0.000003         # $3 per million tokens
  #     output: 0.000015        # $15 per million tokens
  #     context_window: 131072  # 131k tokens
  #   # ... additional models

# Google GenAI Configuration
# Supports both Gemini API (developer-focused) and Vertex AI (enterprise-grade)
google:
  # Backend selection: "gemini" (Gemini API) or "vertexai" (Vertex AI)
  # Auto-detected based on available authentication if not specified
  # backend: "gemini"

  # Authentication for Gemini API
  # api_key: "your-google-api-key"  # Can also use GOOGLE_API_KEY environment variable

  # Authentication for Vertex AI (choose one method)
  # project: "your-gcp-project-id"      # GCP project ID
  # location: "us-central1"             # GCP region (default: us-central1)
  # Using GOOGLE_APPLICATION_CREDENTIALS environment variable is recommended

  # Token budget for thinking capability (Google models support thinking)
  # thinking_budget: 8000  # Default: 8000 tokens



  # Model-specific pricing and configuration (advanced users)
  # pricing:
  #   gemini-2.5-pro:
  #     input: 0.00125            # $1.25 per million tokens (≤200K input)
  #     input_high: 0.0025        # $2.50 per million tokens (>200K input)
  #     output: 0.01              # $10 per million tokens (≤200K input)
  #     output_high: 0.015        # $15 per million tokens (>200K input)
  #     context_window: 2097152   # 2M tokens
  #   gemini-2.5-flash:
  #     input: 0.0003             # $0.30 per million tokens
  #     audio_input: 0.001        # $1.00 per million tokens (audio)
  #     output: 0.0025            # $2.50 per million tokens
  #     context_window: 1048576   # 1M tokens

# Subagent Configuration
# Subagents are spawned as separate processes using shell-out pattern.
# Configure subagent behavior using CLI arguments passed to the subprocess.
#
# subagent_args: CLI arguments to pass when spawning subagents
# Common patterns:
#   - "--use-weak-model"           # Use weak model (same provider as parent)
#   - "--profile cheap"            # Use a different profile
#   - "--profile openai-subagent"  # Use cross-provider profile
#
# Empty value means subagent uses default config (same as parent)
subagent_args: ""

# Example: Use weak model for subagents (cost optimization)
# subagent_args: "--use-weak-model"

# Example: Use a different profile for subagents (cross-provider)
# First define the profile, then reference it:
# profiles:
#   openai-subagent:
#     provider: "openai"
#     model: "gpt-4o-mini"
#     max_tokens: 2048
#
# subagent_args: "--profile openai-subagent"

# Example configurations for common scenarios:

# Scenario 1: Use weak model for subagents (same provider, cost optimization)
# subagent_args: "--use-weak-model"

# Scenario 2: Cross-provider subagents via profiles
# profiles:
#   cheap-openai:
#     provider: "openai"
#     model: "gpt-4o-mini"
#     max_tokens: 2048
#
# subagent_args: "--profile cheap-openai"

# Disable Subagent
# When set to true, the subagent tool is removed from available tools and
# subagent-related context is excluded from the system prompt.
# Other tools like web_fetch and image_recognition remain available.
# Can also be set via the --disable-subagent CLI flag or per-profile.
# disable_subagent: true
#
# Enable Todo Tools
# Todo tools (todo_read and todo_write) are disabled by default for the main agent.
# Set enable_todos to true (or pass --enable-todos) to re-enable them.
# enable_todos: true
#
# Example: Profile with subagent disabled
# profiles:
#   no-subagent:
#     disable_subagent: true

# Scenario 3: Google Vertex AI for enterprise with thinking capability
# provider: "google"
# model: "gemini-2.5-pro"
# thinking_budget_tokens: 8000
# google:
#   backend: "vertexai"
#   project: "my-company-project"
#   location: "us-central1"


# Custom Tools Configuration
# Kodelet can discover and execute custom executable tools from specified directories
custom_tools:
  # Enable/disable custom tools discovery (default: true)
  enabled: true

  # Global custom tools directory (default: ~/.kodelet/tools)
  global_dir: "~/.kodelet/tools"

  # Local custom tools directory (default: ./.kodelet/tools)
  local_dir: "./.kodelet/tools"

  # Execution timeout for custom tools (default: 30s)
  timeout: 30s

  # Maximum output size for custom tools (default: 100KB)
  max_output_size: 102400

  # Tool whitelist - only tools in this list will be loaded (empty means load all tools)
  # When specified, only custom tools with these names will be available
  # tool_white_list:
  #   - "my-custom-tool"
  #   - "another-tool"

# Agentic Skills Configuration
# Skills are model-invoked capabilities that package domain expertise
# Unlike fragments/recipes (user-invoked), skills are automatically invoked by Kodelet
# when relevant to the task. See docs/SKILLS.md for creating custom skills.
skills:
  # Enable/disable skills globally (default: true when not specified)
  # Set to false to completely disable the skills system
  enabled: true

  # Allowlist of skill names (empty = all discovered skills are available)
  # When specified, only these skills will be enabled
  # Skills are discovered from:
  #   - ./.kodelet/skills/<name>/SKILL.md (repository-local, higher precedence)
  #   - ~/.kodelet/skills/<name>/SKILL.md (user-global)
  # allowed:
  #   - pdf
  #   - xlsx
  #   - kubernetes

# Context File Discovery Configuration
# Controls which files are loaded as context for the agent.
# Context files provide project-specific instructions, coding conventions, and guidelines.
context:
  # List of filenames to search for in each directory (default: ["AGENTS.md"])
  # Files are searched in order; first match wins per directory.
  # Common options: AGENTS.md, CLAUDE.md, CODING.md, README.md
  patterns:
    - "AGENTS.md"
  # To also include README.md (not recommended for large READMEs):
  # patterns:
  #   - "AGENTS.md"
  #   - "README.md"

# Tracing Configuration
tracing:
  # Enable OpenTelemetry tracing (default: false)
  enabled: true

  # Sampling strategy (options: always, never, ratio)
  sampler: always

  # Sampling ratio when using ratio sampler (0.0-1.0)
  ratio: 1

# Environment variables can also be used to configure Kodelet:
# - KODELET_LOG_LEVEL: Overrides the log_level setting
# - KODELET_LOG_FORMAT: Overrides the log_format setting
# - KODELET_PROVIDER: Overrides the provider setting (anthropic, openai, google)
# - KODELET_MODEL: Overrides the model setting
# - KODELET_MAX_TOKENS: Overrides the max_tokens setting
# - KODELET_THINKING_BUDGET_TOKENS: Overrides the thinking_budget_tokens setting
# - KODELET_WEAK_MODEL: Overrides the weak_model setting
# - KODELET_WEAK_MODEL_MAX_TOKENS: Overrides the weak_model_max_tokens setting
# - KODELET_REASONING_EFFORT: Overrides the reasoning_effort setting (OpenAI)
# - KODELET_WEAK_REASONING_EFFORT: Overrides the weak_reasoning_effort setting (OpenAI)
# - KODELET_RETRY_ATTEMPTS: Overrides the retry.attempts setting
# - KODELET_RETRY_INITIAL_DELAY: Overrides the retry.initial_delay setting (milliseconds)
# - KODELET_RETRY_MAX_DELAY: Overrides the retry.max_delay setting (milliseconds)
# - KODELET_RETRY_BACKOFF_TYPE: Overrides the retry.backoff_type setting (fixed, exponential)
# - KODELET_ALLOWED_COMMANDS: Comma-separated list of allowed command patterns
# - KODELET_ALLOWED_TOOLS: Comma-separated list of allowed tools for main agent
# - KODELET_DISABLE_SUBAGENT: Disables the subagent tool and related system prompt context (true/false)
# - KODELET_CONTEXT_PATTERNS: Comma-separated list of context file patterns (e.g., "AGENTS.md,README.md")
# - KODELET_TRACING_ENABLED: Enables/disables tracing
# - KODELET_TRACING_SAMPLER: Sets the sampling strategy
# - KODELET_TRACING_RATIO: Sets the sampling ratio
# - KODELET_ALIASES_*: Define individual aliases (e.g., KODELET_ALIASES_SONNET46=claude-sonnet-4-6)
# - KODELET_COMMIT_COAUTHOR_ENABLED: Enable/disable coauthor attribution in commits
# - KODELET_COMMIT_COAUTHOR_NAME: Name for coauthor attribution
# - KODELET_COMMIT_COAUTHOR_EMAIL: Email for coauthor attribution
# - KODELET_OPENAI_USE_RESPONSES_API: Use OpenAI Responses API instead of Chat Completions
#
# API Keys:
# - ANTHROPIC_API_KEY: Required when using the Anthropic provider
# - OPENAI_API_KEY: Required when using the OpenAI provider (default)
# - XAI_API_KEY: Required when using the xAI platform
# - OPENAI_API_BASE: Custom OpenAI API endpoint (overrides config base_url)
# - GOOGLE_API_KEY: Required when using Google Gemini API
# - GEMINI_API_KEY: Alternative to GOOGLE_API_KEY for Gemini API
# - GOOGLE_CLOUD_PROJECT: GCP project ID for Vertex AI
# - GOOGLE_CLOUD_LOCATION: GCP region for Vertex AI (default: us-central1)
# - GOOGLE_APPLICATION_CREDENTIALS: Path to GCP service account JSON for Vertex AI
# - GOOGLE_GENAI_USE_VERTEXAI: Force Vertex AI backend (true/false)
# Note: The API key environment variable can be customized using the api_key_env_var setting
#
# Standard OpenTelemetry environment variables are also supported:
# - OTEL_EXPORTER_OTLP_ENDPOINT: The endpoint to send telemetry data to
# - OTEL_EXPORTER_OTLP_HEADERS: Headers to use when sending telemetry data (for auth)
# - OTEL_RESOURCE_ATTRIBUTES: Additional resource attributes to include

# MCP (Model Context Protocol) Configuration
mcp:
  # Enable/disable MCP tools globally (default: true when not specified)
  # Set to false to completely disable MCP
  enabled: true

  # Execution mode for MCP tools
  # - "direct": Traditional direct tool calling (default)
  # - "code": Code execution with filesystem API (more efficient for multi-tool workflows)
  execution_mode: "direct"

  # Code execution settings (only used when execution_mode = "code")
  code_execution:
    # Directory where MCP tool TypeScript files are generated
    workspace_dir: ".kodelet/mcp"

    # Whether to regenerate tool files on startup
    regenerate_on_startup: false

    # Note: Socket path is now automatically generated per-session as
    # {workspace_dir}/mcp-{session-id}.sock to allow concurrent kodelet instances

  # MCP servers configuration
  # Each server can be either stdio or SSE based
  servers:
    # Example stdio server (uncomment to enable)
    # filesystem:
    #   command: "npx"
    #   args: ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/allowed/directory"]

    # Example SSE server (uncomment to enable)
    # example-sse:
    #   server_type: "sse"
    #   base_url: "https://example.com/mcp"
    #   headers:
    #     Authorization: "Bearer ${MCP_TOKEN}"
    #   tool_white_list: ["tool1", "tool2"]
