# Kodelet - LLM-Friendly Guide

> **Kodelet** is a lightweight agentic SWE Agent that runs as an interactive CLI tool. It performs software engineering and production operations tasks using AI assistance.

## Quick Start

### Installation

```bash
# Using install script
curl -sSL https://raw.githubusercontent.com/jingkaihe/kodelet/main/install.sh | bash

# Update to latest version
kodelet update
```

## Core Usage Modes

### One-shot Mode
Execute single commands quickly:

```bash
# Basic query
kodelet run "your query"

# Continue most recent conversation
kodelet run --follow "continue the task"
kodelet run -f "quick follow-up"

# Resume specific conversation
kodelet run --resume CONVERSATION_ID "more questions"

# Don't save conversation
kodelet run --no-save "temporary query"
```

## Key Features

### Agent Context Files
Kodelet automatically loads project-specific context from `AGENTS.md` (or `KODELET.md` as fallback) in your current directory. This file should contain:

- Project overview and structure
- Tech stack and dependencies
- Engineering principles and coding style
- Key commands (build, test, lint, deploy)
- Configuration and environment setup
- Testing conventions
- Error handling patterns

Bootstrap your context file:
```bash
kodelet run -r init
```

### Fragments/Recipes System
Reusable prompt templates with variable substitution and bash command execution.

**Built-in recipes:**
```bash
kodelet recipe list              # List all available recipes
kodelet recipe show init         # View recipe content

# Use built-in recipes
kodelet run -r init             # Bootstrap AGENTS.md
kodelet run -r commit           # Generate commit messages
kodelet run -r custom-tool      # Create custom tools
```

**Custom recipes:**
Create templates in `./recipes/` or `~/.kodelet/recipes/`:

```markdown
## Context:
Current branch: {{bash "git" "branch" "--show-current"}}
Project: {{.project_name}}

## Task:
Please analyze {{.focus_area}}
```

Usage:
```bash
kodelet run -r my-recipe --arg project_name="Kodelet" --arg focus_area="security"
```

### Git Integration

**AI Commit Messages:**
```bash
git add .
# Recommended: fast, non-interactive, no conversation saved
kodelet commit --short --no-confirm --no-save

# Other options
kodelet commit                  # Interactive with confirmation
kodelet commit --short          # Shorter message with confirmation
kodelet commit --no-confirm     # Skip confirmation only
```

**Pull Requests:**
```bash
kodelet pr                      # Create PR with AI-generated description
kodelet pr --target main        # Specify target branch
```

**Issue Resolution:**
```bash
kodelet issue-resolve --issue-url https://github.com/owner/repo/issues/123
```

**PR Response:**
```bash
kodelet pr-respond --pr-url https://github.com/owner/repo/pull/456
kodelet pr-respond --pr-url URL --review-id 123456
```

### GitHub Actions Background Agent
Automated agent that responds to `@kodelet` mentions:

```bash
kodelet gha-agent-onboard       # Complete setup (GitHub app + secrets + workflow)
```

After setup, team members can mention `@kodelet` in issues and PRs for automated assistance.

### Image Input Support
Vision-enabled analysis with multi-modal models:

```bash
# Single image
kodelet run --image /path/to/screenshot.png "What's wrong with this UI?"

# Multiple images
kodelet run --image ./diagram.png --image ./mockup.jpg "Compare these designs"
```

**Supported:** JPEG, PNG, GIF, WebP | **Max:** 5MB per image, 10 images per message
**Provider Support:** All providers (Anthropic, OpenAI, Google) if the model supports multi-modal

### Custom Tools
Extend kodelet with executable tools in any language:

**Directory structure:**
- `~/.kodelet/tools/` - Global tools
- `./kodelet-tools/` - Project-specific tools

**Generate custom tool:**
```bash
kodelet run -r custom-tool --arg task="fetch weather without API key"
kodelet run -r custom-tool --arg task="validate JSON" --arg global=true
```

**Tool protocol:**
```bash
./my-tool description  # Returns JSON schema
./my-tool run          # Executes with JSON input from stdin
```

### MCP Integration
Model Context Protocol for external integrations. Configure in `config.yaml`:

```yaml
mcp:
  servers:
    fs:
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-filesystem", "/path"]
      tool_white_list: ["list_directory"]
```

## Conversation Management

```bash
# List conversations
kodelet conversation list
kodelet conversation list --search "keyword"

# View conversation
kodelet conversation show <id>
kodelet conversation show <id> --format json

# Stream conversation (real-time)
kodelet conversation stream <id>

# Delete conversation
kodelet conversation delete <id>

# Fork conversation (experimental branching)
# 1. Ensure clean git status
# 2. Fork the conversation to try different approaches
# 3. If it doesn't work: git reset --hard and continue with original
kodelet conversation fork <id>
```

## Configuration

Kodelet uses layered configuration:
1. Global: `~/.kodelet/config.yaml`
2. Repository: `./kodelet-config.yaml` (overrides global)

Example `config.yaml`:
```yaml
aliases:
    gemini-flash: gemini-2.5-flash
    gemini-pro: gemini-2.5-pro
    haiku-35: claude-3-5-haiku-20241022
    opus-41: claude-opus-4-1-20250805
    sonnet-45: claude-sonnet-4-5-20250929
max_tokens: 16000
model: sonnet-45
profile: default
thinking_budget_tokens: 8000
weak_model: haiku-35
weak_model_max_tokens: 8192
profiles:
    hybrid:
        max_tokens: 16000
        model: sonnet-45
        subagent:
            allowed_tools:
                - file_read
                - glob_tool
                - grep_tool
            model: gpt-5
            provider: openai
            reasoning_effort: high
        thinking_budget_tokens: 8000
        weak_model: haiku-35
        weak_model_max_tokens: 8192
    openai:
        max_tokens: 16000
        model: gpt-5
        provider: openai
        reasoning_effort: medium
        weak_model: gpt-5
    premium:
        max_tokens: 16000
        model: opus-41
        thinking_budget_tokens: 8000
        weak_model: sonnet-45
        weak_model_max_tokens: 8192
    google:
        max_tokens: 16000
        model: gemini-pro
        provider: google
        weak_model: gemini-flash
        weak_model_max_tokens: 8192
    xai:
        max_tokens: 16000
        model: grok-code-fast-1
        openai:
            preset: xai
        provider: openai
        reasoning_effort: none
        weak_model: grok-code-fast-1
```

## LLM Providers

### Anthropic Claude (Recommended)
**Models:**
- `claude-sonnet-4-5-20250929` - Recommended for standard tasks
- `claude-3-5-haiku-20241022` - Lightweight tasks
- `claude-opus-4-1-20250805` - Complex reasoning

**Features:**
- Vision capabilities
- Message caching
- Extended thinking mode

**Setup:**
```bash
# Option 1: Using anthropic-login (no environment variable needed)
kodelet anthropic-login

# Option 2: Using environment variable
export ANTHROPIC_API_KEY="sk-ant-api..."

# Use the provider
kodelet run --provider anthropic "query"
```

### OpenAI
**Models:**
- `gpt-5` - Latest GPT model

**Features:**
- Reasoning effort control (low, medium, high)
- Function calling
- Vision capabilities (multi-modal models)

**Setup:**
```bash
export OPENAI_API_KEY="sk-..."
kodelet run --provider openai --model gpt-5 "query"
```

### Google Gemini
**Models:**
- `gemini-2.5-pro` - Advanced capabilities
- `gemini-2.5-flash` - Fast responses

**Features:**
- Vision capabilities (multi-modal models)
- Fast inference

**Setup:**
```bash
export GOOGLE_API_KEY="your-api-key"
kodelet run --provider google --model gemini-2.5-pro "query"
```

## Advanced Features

### Streaming and Programmatic Access
Headless mode outputs structured JSON stream:

```bash
# Stream JSON output
kodelet run --headless "analyze this codebase"
kodelet run --headless --include-history "continue analysis"

# Real-time conversation stream
kodelet conversation stream CONVERSATION_ID
kodelet conversation stream CONVERSATION_ID --include-history
```

**StreamEntry JSON format:**
```json
{"kind":"text","role":"user","content":"What files are here?","conversation_id":"conv_123"}
{"kind":"thinking","role":"assistant","content":"User wants to see files..."}
{"kind":"tool-use","tool_name":"bash","input":"{\"command\":\"ls\"}","tool_call_id":"call_456"}
{"kind":"tool-result","tool_name":"bash","result":"file1.txt\nfile2.go"}
{"kind":"text","role":"assistant","content":"Here are the files..."}
```

**Processing with jq:**
```bash
# Extract text only
kodelet run --headless "query" | jq -r 'select(.kind == "text") | .content'

# Monitor tool usage
kodelet conversation stream ID | jq 'select(.kind == "tool-use") | .tool_name'
```

### Web UI
Browser-based interface:

```bash
kodelet serve                   # Start on localhost:8080
kodelet serve --host 0.0.0.0 --port 3000
```

Access at `http://localhost:8080` for conversation management and chat interface.

### Feedback System
Send feedback to conversations:

```bash
kodelet feedback --follow "great job, but please add tests"
kodelet feedback --conversation-id ID "needs improvement on error handling"
```

### Shell Completion
Tab completion for bash, zsh, fish:

```bash
# Bash
echo 'source <(kodelet completion bash)' >> ~/.bashrc

# Zsh
echo 'source <(kodelet completion zsh)' >> ~/.zshrc

# Fish
kodelet completion fish > ~/.config/fish/completions/kodelet.fish
```

## Security & Best Practices

### Command Restrictions
Control which bash commands kodelet can execute:

```yaml
# config.yaml
allowed_commands:
  - "ls *"
  - "pwd"
  - "git status"
  - "npm *"
```

Or via environment:
```bash
export KODELET_ALLOWED_COMMANDS="ls *,pwd,git status"
```

### Tool Restrictions
Limit available tools:

```bash
kodelet run --allowed-tools "file_read,grep_tool,bash" "analyze code"
```

### Best Practices
1. **Use context files** (`AGENTS.md`) for project-specific conventions
2. **Create fragments** for repetitive tasks
3. **Utilize profiles** for different model configurations
4. **Enable conversation persistence** to build on previous work
5. **Use `--follow` flag** to continue conversations efficiently
6. **Restrict commands** in automated environments
7. **Review AI-generated code** before committing

## Common Workflows

### Code Review
```bash
git diff main | kodelet run "review these changes for issues"
```

### Bug Investigation
```bash
kodelet run "analyze error logs and suggest fixes"
kodelet run --follow "implement the suggested fix"
```

### Refactoring
```bash
kodelet run "refactor user authentication to use middleware pattern"
```

### Documentation
```bash
kodelet run "add comprehensive docstrings to all functions in this file"
```

### Testing
```bash
kodelet run "write unit tests for the payment processing module"
```

## Resources

- **Website:** https://kodelet.com
- **GitHub:** https://github.com/jingkaihe/kodelet
- **Documentation:** https://github.com/jingkaihe/kodelet/tree/main/docs

## Version Information

```bash
kodelet version                 # Show version and build info
```

---

**Note:** This guide focuses on end-user usage. For development information, build processes, and contribution guidelines, see the project's GitHub repository and development documentation.

